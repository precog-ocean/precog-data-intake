{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a53c5f73e4768d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a self-guided step-by-step Jupyter notebook guide that will show you how to run Python scripts saved on the ```/scripts``` directory of the [precog-data-intake](https://github.com/precog-ocean/precog-data-intake) repository.\n",
    "The scripts will be run from within this Jupyter notebook for illustrative purposes.\n",
    "\n",
    "\n",
    "To run each ```.py``` script, simply run each cell in this notebook and follow the in-cell prompts as if you were working on any ```Terminal prompt```.\n",
    "\n",
    "You can also run equivalent commands on any ```Terminal prompt``` with slightly different semantics (shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fc9d723c1e8ba",
   "metadata": {},
   "source": [
    "## Step 1. Check / Activate Python Environment\n",
    "Type the following command to activate the virtual environment from within your Jupyter notebook server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e145466b49f157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:35:24.499436Z",
     "start_time": "2026-02-04T17:35:24.462479Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1a699-7ec7-4ac9-b410-386168f1368a",
   "metadata": {},
   "source": [
    "The command above is similar to running the following on the ```Terminal prompt```:\n",
    "```bash \n",
    "source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167590606f66a25",
   "metadata": {},
   "source": [
    "## Step 2. Create directory to save search results\n",
    "\n",
    "Create a directory named ```test_search``` under your Desktop.\n",
    "\n",
    "PS. This is equivalent to running ```> mkdir ~/Desktop/test_search ``` on a ```Terminal prompt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13a061b92300dc0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:35:26.468953Z",
     "start_time": "2026-02-04T17:35:26.376468Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/Desktop/test_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c02f6340c0376",
   "metadata": {},
   "source": [
    "## Step 3. ESGF Catalogue sweep for ESM outputs of interest\n",
    "\n",
    "Now run the next cell in the notebook to execute the program ```intake_CatalogueSearch.py``` and follow the in-prompt instructions. When asked to provied `variable_ids` insert  `['expc', 'epc100']`\n",
    "\n",
    "PS. This is equivalent to running the command ```> python scripts/intake_CatalogueSearch.py``` on a ```Terminal prompt```"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f87888683383d5e",
   "metadata": {},
   "source": [
    "import os.path\n",
    "%run scripts/intake_CatalogueSearch.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "178d25a0-45fe-451c-ab41-f52a8c8152de",
   "metadata": {},
   "source": [
    "Now look at the ```path``` you indicated and inspect the files created. You should have the following:\n",
    "\n",
    "- ESGF_search_<datetime_stamp>.xlsx ==> This is a Dataframe with the raw search results from all ESGF nodes.\n",
    "- ESGF_search_<datetime_stamp>_<varstamp>.log ==> This is a text file with the log results from the ESGF sweep and also contains results for grid consistency tests as well as continuity of time stamps in files. \n",
    "- DF_Downloadable_XXX.xlsx ==> this is a Dataframe with the filtered and tested URLs for the variables you conducted the search for.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For instance, if you inspect the log file, it shows that complete PI and Historical runs for both  `epc100` and `expc` where found for the following CMIP6 models:\n",
    "```\n",
    "- GFDL-ESM4\n",
    "- GISS-E2-1-G\n",
    "- IPSL-CM6A-LR\n",
    "- MPI-ESM-1-2-HAM\n",
    "- MPI-ESM1-2-HR\n",
    "- MPI-ESM1-2-LR\n",
    "- UKESM1-0-LL\n",
    "```\n",
    "\n",
    "It also shows in a readable format test results where, for example, model `GISS-E2-1-G-CC` has availability for `expc` in a native grid (i.e., `gn`) for the piControl run but is lacking `expc` outputs for the Historical run in the native grid.\n",
    "\n",
    "```\n",
    "No complete set of variables for model GISS-E2-1-G-CC for variable ['expc', 'epc100'] in either grid. Test returned:\n",
    "INFO - ####################################################################################################\n",
    "INFO - grid_label       var_test   variable_ids  has_all_variables        run          model\n",
    "INFO -         gn  [True, False] [expc, epc100]              False  piControl GISS-E2-1-G-CC\n",
    "INFO -         gr [False, False] [expc, epc100]              False  piControl GISS-E2-1-G-CC\n",
    "INFO -         gn  [False, True] [expc, epc100]              False historical GISS-E2-1-G-CC\n",
    "INFO -         gr [False, False] [expc, epc100]              False historical GISS-E2-1-G-CC\n",
    "INFO - ####################################################################################################\n",
    "```"
   ],
   "id": "83ce1666753b417f"
  },
  {
   "cell_type": "markdown",
   "id": "5b72aba2-9587-4e0a-8014-283b4ed93df9",
   "metadata": {},
   "source": [
    "### Step 3.1\n",
    "\n",
    "For the sake of illustration, open ```DF_Downloadable_expc_epc100.xlsx``` and delete all rows but the top 5. Otherwise, it will trigger a 719Gb download to your disk once you run the program intended to trigger data fetching on [Step4](##-Step-4.-Fetch-the-data).\n",
    "\n",
    "You can do so by running this short program below:"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e516f10-1763-4e81-b56a-0ef4c4118519",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def keep_top_five_rows(path: Path, sheet_name):\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(path, sheet_name=0)  # sheet_name=0 = first sheet\n",
    "    # Keep only the first 5 rows\n",
    "    df_top5 = df.head(5)\n",
    "    # Overwrite the original file (no index column)\n",
    "    df_top5.to_excel(path, sheet_name=sheet_name, index=False)\n",
    "    print(f\"File {excel_path} has been trimmed.\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Path to your file\n",
    "    excel_path = input(f\"Now either drag onto terminal or type path to Dataframe with the Filtered ESGF search results:\")\n",
    "    excel_path = Path(excel_path.strip(\" \"))  # strip needed as dragging onto terminal adds a trailing 'space'\n",
    "    keep_top_five_rows(excel_path, sheet_name='Sheet1')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "049102f4-e074-46c1-9914-886bad0301c2",
   "metadata": {},
   "source": [
    "You can use the same logic from the cell above to edit the script and add some custom filtering and produce new Dataframe files by combining multiple criteria before passing the `DF_Downloadable_<varstamp>.xlsx` dataframe to the program responsible for fetching data `intake_OceanVarsDL.py`.\n",
    "\n",
    "You could, for instance, just download `UKESM-1-0-LL` by tweaking the code to:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T22:02:49.737379Z",
     "start_time": "2026-02-05T22:02:46.446179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def filter_model(path, sheet_name, model):\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(path, sheet_name=0)  # sheet_name=0 = first sheet\n",
    "    fname= path.name.split('.')[0] + '_' + model + '.xlsx'\n",
    "    # Keep only the model you want\n",
    "    df_model = df[df['source_id']==model]\n",
    "    # Export filtered file\n",
    "    df_model.to_excel(os.path.join(path.parent, fname), sheet_name=sheet_name, index=False)\n",
    "    print(f\"File {excel_path} has been trimmed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your file\n",
    "    excel_path = input(f\"Now either drag onto terminal or type path to Dataframe with the Filtered ESGF search results:\")\n",
    "    excel_path = Path(excel_path.strip(\" \"))  # strip needed as dragging onto terminal adds a trailing 'space'\n",
    "    filter_model(excel_path, sheet_name='Sheet1', model='UKESM1-0-LL')"
   ],
   "id": "3c6a11a79ebb0468",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/leonardobertini/Desktop/DF_Downloadable_expc_epc100.xlsx has been trimmed.\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "665c7ab81b494c46",
   "metadata": {},
   "source": [
    "## Step 4. Fetch the data\n",
    "\n",
    "The next step is to run the downloader script.\n",
    "\n",
    "The program will download the filtered search results from the ```ESGF_search_<varstamp>.xlsx``` Dataframe.\n",
    "\n",
    "You can indicate where you'd like files to be downloaded to or keep ```~/Desktop/search_results```  created on [Step 2](##-Step-2.-Create-directory-to-save-search-results) as your default.\n",
    "\n",
    "Downloads will trigger in parallel, and files will be organised under a directory tree that has a directory named ```CMIP6``` at the top.\n",
    "\n",
    "Run the following cell:\n",
    "\n",
    "PS. This is equivalent to running the command ```> python scripts/intake_OcanVarsDL.py``` on a ```Terminal prompt```\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "edb4d41168be1490",
   "metadata": {},
   "source": [
    "%run scripts/intake_OceanVarsDL.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aebcc321-1609-41be-b399-7ef2a0bb1c58",
   "metadata": {},
   "source": [
    "When the program finishes running, a folder ```CMIP6``` should have been created within your ```downlaod_path``` with the data organised per model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e88faa0778d17d",
   "metadata": {},
   "source": [
    "## Step 5. Fetch Grid cell measures (`areacello` and `volcello`)\n",
    "\n",
    "Run the next script to fetch corresponding grid cell measures ```areacello``` and ```volcello``` for the downloaded ESM outputs.\n",
    "\n",
    "The program will fetch the grid cell measures and will create a new dataframe ```DF_Downloadable_<cellmeasure_stamp>.xlsx``` on the chosen ```download_path```.\n",
    "\n",
    "Then you'll be prompted to indicate the path to this newly created dataframe, and the cell measure downloads will trigger in parallel.\n",
    "\n",
    "Files will be organised under a directory tree that has a directory ```CMIP6``` at the top.\n",
    "\n",
    "Run the following cell:\n",
    "\n",
    "PS. This is equivalent to running the command ```> python scripts/intake_CellMeasuresDL.py``` on a ```Terminal prompt```"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c0ecee05d91355",
   "metadata": {},
   "source": [
    "%run scripts/intake_CellMeasuresDL.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cef2849-3170-4259-b343-37d882cc329c",
   "metadata": {},
   "source": [
    "## Step 6. Check files\n",
    "\n",
    "That's it. Now inspect `download_path` to check if the files were downloaded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

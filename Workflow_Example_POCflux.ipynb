{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a53c5f73e4768d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a self-guided step-by-step Jupyter notebook guide that will show you how to run Python scripts saved on the ```/scripts``` directory of the [precog-data-intake](https://github.com/precog-ocean/precog-data-intake) repository.\n",
    "The scripts will be run from within this Jupyter notebook for illustrative purposes.\n",
    "\n",
    "\n",
    "To run each ```.py``` script, simply run each cell in this notebook and follow the in-cell prompts as if you were working on any ```Terminal prompt```.\n",
    "\n",
    "You can also run equivalent commands on any ```Terminal prompt``` with slightly different semantics (shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fc9d723c1e8ba",
   "metadata": {},
   "source": [
    "## Step 1. Check / Activate Python Environment\n",
    "Type the following command to activate the virtual environment from within your Jupyter notebook server:"
   ]
  },
  {
   "cell_type": "code",
   "id": "68e145466b49f157",
   "metadata": {},
   "source": [
    "%%bash\n",
    "source .venv/bin/activate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9f1a699-7ec7-4ac9-b410-386168f1368a",
   "metadata": {},
   "source": [
    "The command above is similar to running the following on the ```Terminal prompt```:\n",
    "```bash \n",
    "source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167590606f66a25",
   "metadata": {},
   "source": [
    "## Step 2. Create directory to save search results\n",
    "\n",
    "Create a directory named ```test_search``` under your Desktop.\n",
    "\n",
    "PS. This is equivalent to running ```> mkdir ~/Desktop/test_search ``` on a ```Terminal prompt```"
   ]
  },
  {
   "cell_type": "code",
   "id": "13a061b92300dc0e",
   "metadata": {},
   "source": [
    "%%bash\n",
    "mkdir -p ~/Desktop/test_search"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd6c02f6340c0376",
   "metadata": {},
   "source": [
    "## Step 3. ESGF Catalogue sweep for ESM outputs of interest\n",
    "\n",
    "Now run the next cell in the notebook to execute the program ```intake_CatalogueSearch.py``` and follow the in-prompt instructions. When asked to provied `variable_ids` insert  `['expc', 'epc100']`\n",
    "\n",
    "PS. This is equivalent to running the command ```> python scripts/intake_CatalogueSearch.py``` on a ```Terminal prompt```"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f87888683383d5e",
   "metadata": {},
   "source": [
    "import os.path\n",
    "%run scripts/intake_CatalogueSearch.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "178d25a0-45fe-451c-ab41-f52a8c8152de",
   "metadata": {},
   "source": [
    "Now look at the ```path``` you indicated and inspect the files created. You should have the following:\n",
    "\n",
    "- `ESGF_search_<datetime_stamp>.xlsx` ==> This is a Dataframe with the raw search results from all ESGF nodes.\n",
    "- `ESGF_search_<datetime_stamp>_<varstamp>.log.txt` ==> This is a text file with the log results from the ESGF sweep and also contains results for grid consistency tests as well as continuity of time stamps in files.\n",
    "- `DF_Downloadable_XXX.xlsx` ==> this is a Dataframe with the filtered and tested URLs for the variables you conducted the search for.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For instance, if you inspect the log file, it shows that complete PI and Historical runs for both  `epc100` and `expc` where found for the following CMIP6 models:\n",
    "```\n",
    "- GFDL-ESM4\n",
    "- GISS-E2-1-G\n",
    "- IPSL-CM6A-LR\n",
    "- MPI-ESM-1-2-HAM\n",
    "- MPI-ESM1-2-HR\n",
    "- MPI-ESM1-2-LR\n",
    "- UKESM1-0-LL\n",
    "```\n",
    "\n",
    "It also shows in a readable format test results where, for example, model `GISS-E2-1-G-CC` has availability for `expc` in a native grid (i.e., `gn`) for the piControl run but is lacking `expc` outputs for the Historical run in the native grid.\n",
    "\n",
    "```\n",
    "No complete set of variables for model GISS-E2-1-G-CC for variable ['expc', 'epc100'] in either grid. Test returned:\n",
    "INFO - ####################################################################################################\n",
    "INFO - grid_label       var_test   variable_ids  has_all_variables        run          model\n",
    "INFO -         gn  [True, False] [expc, epc100]              False  piControl GISS-E2-1-G-CC\n",
    "INFO -         gr [False, False] [expc, epc100]              False  piControl GISS-E2-1-G-CC\n",
    "INFO -         gn  [False, True] [expc, epc100]              False historical GISS-E2-1-G-CC\n",
    "INFO -         gr [False, False] [expc, epc100]              False historical GISS-E2-1-G-CC\n",
    "INFO - ####################################################################################################\n",
    "```"
   ],
   "id": "83ce1666753b417f"
  },
  {
   "cell_type": "markdown",
   "id": "5b72aba2-9587-4e0a-8014-283b4ed93df9",
   "metadata": {},
   "source": [
    "### Step 3.1\n",
    "\n",
    "The dataframe `DF_Downloadable_expc_epc100.xlsx` has entries for all the shortlisted models.\n",
    "\n",
    "You can add some custom filtering and produce new Dataframe files by combining multiple criteria before passing the dataframe `DF_Downloadable_<varstamp>.xlsx` to the program responsible for fetching data `intake_OceanVarsDL.py`.\n",
    "\n",
    "You could, for instance, create a second dataframe containing only `UKESM1-0-LL` entries by running the following sub program:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def filter_model(path, sheet_name, model):\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(path, sheet_name=0)  # sheet_name=0 = first sheet\n",
    "    fname= path.name.split('.')[0] + '_' + model + '.xlsx'\n",
    "    # Keep only the model you want\n",
    "    df_model = df[df['source_id']==model]\n",
    "    # Export filtered file\n",
    "    df_model.to_excel(os.path.join(path.parent, fname), sheet_name=sheet_name, index=False)\n",
    "    print(f\"File {excel_path} has been trimmed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your file\n",
    "    excel_path = input(f\"Now either drag onto terminal or type path to Dataframe with the Filtered ESGF search results:\")\n",
    "    excel_path = Path(excel_path.strip(\" \"))  # strip needed as dragging onto terminal adds a trailing 'space'\n",
    "    filter_model(excel_path, sheet_name='Sheet1', model='UKESM1-0-LL')"
   ],
   "id": "3c6a11a79ebb0468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the sake of illustration, you can use the same logic from the sub program above so that it will delete all rows but the top 5 on this `UKESM1-0-LL` dataframe. Otherwise, it would prompt you to accept a 200Gb download of many files to your disk once you run the program that will fetch data on __Step 4__.\n",
    "\n",
    "You can do so by running this sub program below:"
   ],
   "id": "c7971d77d534aa6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def keep_top_five_rows(path: Path, sheet_name):\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(path, sheet_name=0)  # sheet_name=0 = first sheet\n",
    "    # Keep only the first 5 rows\n",
    "    df_top5 = df.head(5)\n",
    "    # Overwrite the original file (no index column)\n",
    "    df_top5.to_excel(path, sheet_name=sheet_name, index=False)\n",
    "    print(f\"File {excel_path} has been trimmed.\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Path to your file\n",
    "    excel_path = input(f\"Now either drag onto terminal or type path to Dataframe with the Filtered ESGF search results:\")\n",
    "    excel_path = Path(excel_path.strip(\" \"))  # strip needed as dragging onto terminal adds a trailing 'space'\n",
    "    keep_top_five_rows(excel_path, sheet_name='Sheet1')"
   ],
   "id": "9e915c0ea76a76b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "665c7ab81b494c46",
   "metadata": {},
   "source": [
    "## Step 4. Fetch the data\n",
    "\n",
    "The next step is to run the downloader script.\n",
    "The program will download the filtered search results from the ```ESGF_search_<varstamp>.xlsx``` Dataframe.\n",
    "\n",
    "You can indicate where you'd like files to be downloaded to or keep ```~/Desktop/search_results```  created on __Step 2__ as your default.\n",
    "\n",
    "Downloads will trigger in parallel, and files will be organised under a directory tree that has a directory named ```CMIP6``` at the top.\n",
    "\n",
    "Run the following cell:\n",
    "\n",
    "PS. This is equivalent to running the command ```> python scripts/intake_OcanVarsDL.py``` on a ```Terminal prompt```\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "edb4d41168be1490",
   "metadata": {},
   "source": [
    "%run scripts/intake_OceanVarsDL.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aebcc321-1609-41be-b399-7ef2a0bb1c58",
   "metadata": {},
   "source": [
    "When the program finishes running, a folder ```CMIP6``` should have been created within your ```downlaod_path``` with the data organised per model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e88faa0778d17d",
   "metadata": {},
   "source": [
    "## Step 5. Fetch Grid cell measures (`areacello` and `volcello`)\n",
    "\n",
    "Run the next script to fetch corresponding grid cell measures ```areacello``` and ```volcello``` for the downloaded ESM outputs.\n",
    "\n",
    "The program will fetch the grid cell measures and will create a new dataframe ```DF_Downloadable_<cellmeasure_stamp>.xlsx``` on the chosen ```download_path```.\n",
    "\n",
    "Then you'll be prompted to indicate the path to this newly created dataframe, and the cell measure downloads will trigger in parallel.\n",
    "\n",
    "Files will be organised under a directory tree that has a directory ```CMIP6``` at the top.\n",
    "\n",
    "Run the following cell:\n",
    "\n",
    "PS. This is equivalent to running the command ```> python scripts/intake_CellMeasuresDL.py``` on a ```Terminal prompt```"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c0ecee05d91355",
   "metadata": {},
   "source": [
    "%run scripts/intake_CellMeasuresDL.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cef2849-3170-4259-b343-37d882cc329c",
   "metadata": {},
   "source": [
    "## Step 6. Check files\n",
    "\n",
    "That's it. Now inspect `download_path` to check if the files were downloaded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
